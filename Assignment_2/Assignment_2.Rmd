---
title: "Assignment_2_KNN"
author: "Tirumal Achina"
date: "2023-09-29"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
***
# Summary:




1.How would this customer be classified? This new customer would be classified as 0, does not take the personal loan

2.The Optimal K is 3.

3.The whole data is validated with the best k value(i.e; k = 3).

4.The customer with the credentials has been evaluated with max k value and the model assigned it as '0' that implies does not take the loan.

5.Here we repartitioned the data accordingly and applied k-NN method to the best value of k.Now, let's compare the key metrics and statistics:


##Accuracy:

-Training Set: 0.9764

-Validation Set: 0.968

-Test Set: 0.961

The training set's accuracy is slightly greater than the validation and test sets', but all sets' accuracy is high, showing that the model performs well in terms of overall correctness.

##Sensitivity (True Positive Rate):

-Training Set: 0.9978

-Validation Set: 0.9956

-Test Set: 0.9955

Sensitivity assesses the model's ability to detect the positive class (in this case, class 1) properly. All of the sets have extremely high sensitivity, indicating that the model is highly good at spotting class 1 cases.


##Specificity (True Negative Rate):

-Training Set: 0.7672

-Validation Set: 0.6912

-Test Set: 0.6875

Specificity measures the model's ability to correctly identify the negative class (in this case, class 0). The training set has the highest specificity, while the test and validation sets have lower specificity values, suggesting that the model is less accurate at correctly identifying class 0 instances.

##Positive Predictive Value (Precision):

-Training Set: 0.9767

-Validation Set: 0.9700

-Test Set: 0.9619

Precision is the proportion of true positive predictions made by the model out of all positive predictions made by the model. The values are consistent across all sets, implying a reasonable balance of precision and recall.

The model performs admirably across all three, with only minor variations in performance between the training, validation, and test sets. But as you move from the training set to the validation and test sets, the specificity starts to noticeably decline. This demonstrates that the model may be more prone to false positives on unknown data—predicting class 1 when it is actually class 0—than on known data. Specificity on the test set may be improved by further fine-tuning the model's parameters, such as altering the classification threshold or experimenting with different values of k (if applicable). If at all possible, take into account testing the model's performance using more varied or representative data. 



***

```{r}
#Loading the libraries that are required for the task
library(class)
library(caret)
library(e1071)
```
Read the data.

```{r }
bank <- read.csv("UniversalBank.csv")
dim(bank)
t(t(names(bank))) # The t function creates a transpose of the dataframe
```
Drop ID and ZIP
```{r}
bank <- bank[,-c(1,5)]
```

Split Data into 60% training and 40% validation. There are many ways to do this. We will look at 2 different ways. Before we split, let us transform categorical variables into dummy variables

```{r}
# Only Education needs to be converted to factor
bank$Education <- as.factor(bank$Education)

# Now, convert Education to Dummy Variables

groups <- dummyVars(~., data = bank) # This creates the dummy groups
universal_m.df <- as.data.frame(predict(groups,bank))


set.seed(1)  # Important to ensure that we get the same sample if we rerun the code
train.index <- sample(row.names(universal_m.df), 0.6*dim(universal_m.df)[1])
valid.index <- setdiff(row.names(universal_m.df), train.index)  
train.df <- universal_m.df[train.index,]
valid.df <- universal_m.df[valid.index,]
t(t(names(train.df)))

#Second approach

library(caTools)
set.seed(1)
split <- sample.split(universal_m.df, SplitRatio = 0.6)
training_set <- subset(universal_m.df, split == TRUE)
validation_set <- subset(universal_m.df, split == FALSE)

# Print the sizes of the training and validation sets
print(paste("The size of the training set is:", nrow(training_set)))
print(paste("The size of the validation set is:", nrow(validation_set)))
```

Now, let us normalize the data
```{r}
train.norm.df <- train.df[,-10] # Note that Personal Income is the 10th variable
valid.norm.df <- valid.df[,-10]

norm.values <- preProcess(train.df[, -10], method=c("center", "scale"))
train.norm.df <- predict(norm.values, train.df[, -10])
valid.norm.df <- predict(norm.values, valid.df[, -10])
```

### Questions

Consider the following customer:

1. Age = 40, Experience = 10, Income = 84, Family = 2, CCAvg = 2, Education_1 = 0, Education_2 =1, Education_3 = 0, Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1, and Credit Card = 1. Perform a k-NN classification with all predictors except ID and ZIP code using k = 1. Remember to transform categorical predictors with more than two categories into dummy variables first. Specify the success class as 1 (loan acceptance), and use the default cutoff value of 0.5. How would this customer be classified?

```{r}
# We have converted all categorical variables to dummy variables
# Let's create a new sample
new_customer <- data.frame(
  Age = 40,
  Experience = 10,
  Income = 84,
  Family = 2,
  CCAvg = 2,
  Education.1 = 0,
  Education.2 = 1,
  Education.3 = 0,
  Mortgage = 0,
  Securities.Account = 0,
  CD.Account = 0,
  Online = 1,
  CreditCard = 1
)

# Normalize the new customer
new.cust.norm <- new_customer
new.cust.norm <- predict(norm.values, new.cust.norm)

```

Now, let us predict using knn
```{r}

knn.pred1 <- class::knn(train = train.norm.df, 
                       test = new.cust.norm, 
                       cl = train.df$Personal.Loan, k = 1)
knn.pred1

```

***

2. What is a choice of k that balances between overfitting and ignoring the predictor
information?

```{r}
# Calculate the accuracy for each value of k
# Set the range of k values to consider

accuracy.df <- data.frame(k = seq(1, 20, 1), overallaccuracy = rep(0, 20))
for(i in 1:20) {
  knn.pred <- class::knn(train = train.norm.df, 
                         test = valid.norm.df, 
                         cl = train.df$Personal.Loan, k = i)
  accuracy.df[i, 2] <- confusionMatrix(knn.pred, 
                                       as.factor(valid.df$Personal.Loan),positive = "1")$overall[1]
}
accuracy.df
which(accuracy.df[,2] == max(accuracy.df[,2])) 

plot(accuracy.df$k,accuracy.df$overallaccuracy)

```

3. Show the confusion matrix for the validation data that results from using the best k.
```{r}

prediction_knn <- knn(train = train.norm.df, test = valid.norm.df,cl = train.df$Personal.Loan, k = 3, prob=TRUE)

confusionMatrix(prediction_knn,as.factor(valid.df$Personal.Loan))
```


4. Consider the following customer: Age = 40, Experience = 10, Income = 84,
Family = 2, CCAvg = 2, Education_1 = 0, Education_2 = 1, Education_3 = 0,
Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1 and Credit
Card = 1. Classify the customer using the best k.

```{r}
customer <- data.frame(
  Age = 40,
  Experience = 10,
  Income = 84,
  Family = 2,
  CCAvg = 2,
  Education.1 = 0,
  Education.2 = 1,
  Education.3 = 0,
  Mortgage = 0,
  Securities.Account = 0,
  CD.Account = 0,
  Online = 1,
  CreditCard = 1
)

# Normalize the new customer
new.cust.norm <- customer
new.cust.norm <- predict(norm.values, new.cust.norm)

knn.pred1 <- class::knn(train = train.norm.df, 
                       test = new.cust.norm, 
                       cl = train.df$Personal.Loan, k = 3)
knn.pred1
```

***
5.Repartition the data, this time into training, validation, and test sets (50% : 30% : 20%). Apply the k-NN method with the k chosen above. Compare the confusion matrix of the test set with that of the training and validation sets. Comment on the differences and their reason
***
```{r}
#Repartitioning the training, validation and test sets to 50,30, and 20 percents.
set.seed(1)
train.index = sample(row.names(universal_m.df), 0.5*dim(universal_m.df)[1])
remaining.index = setdiff(row.names(universal_m.df),train.index)
valid.index = sample(remaining.index,0.3*dim(universal_m.df)[1])
test.index = setdiff(remaining.index,valid.index)

#Loading the partitioned dets into the dataframe.
train.df = universal_m.df[train.index,]
valid.df= universal_m.df[valid.index,]
test.df = universal_m.df[test.index,]

#Normalizing the data after repartitioning accordingly.

train.norm.df <- train.df[, -10]  
valid.norm.df <- valid.df[, -10]
test.norm.df <- test.df[, -10]

norm.values <- preProcess(train.df[, -10], method = c("center", "scale"))
train.norm.df <- predict(norm.values, train.df[, -10])
valid.norm.df <- predict(norm.values, valid.df[, -10])
test.norm.df <- predict(norm.values, test.df[, -10])

#Applying the k-NN method to all the sets that we have. As requires we are keeping the k value that we used in the previous question that is max of K.
#Confusion matrix that gives all the data that are correctly identified and wrongly identified.

#Training set
knn_t <- class::knn(train = train.norm.df,test = train.norm.df, cl = train.df$Personal.Loan, k = 3)
confusionMatrix(knn_t, as.factor(train.df$Personal.Loan))

#Validation set
knn_v <- class::knn(train = train.norm.df,test = valid.norm.df,cl = train.df$Personal.Loan, k = 3)
confusionMatrix(knn_v, as.factor(valid.df$Personal.Loan))

#Test set
knn_ts <- class::knn(train = train.norm.df,test = test.norm.df, cl = train.df$Personal.Loan, k = 3)
confusionMatrix(knn_ts, as.factor(test.df[,10]))


```


